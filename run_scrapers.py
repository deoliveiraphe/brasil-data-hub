#!/usr/bin/env python3
"""
Script principal para executar todos os scrapers do Brasil Data Hub.

Este script:
1. Limpa todas as tabelas do banco de dados
2. Executa todos os scrapers em sequÃªncia
3. Gera relatÃ³rio final com estatÃ­sticas
4. Salva logs detalhados de execuÃ§Ã£o

Usage:
    python run_scrapers.py                    # Executa todos os scrapers
    python run_scrapers.py --scraper private # Executa apenas aerÃ³dromos privados
    python run_scrapers.py --scraper public  # Executa apenas aerÃ³dromos pÃºblicos
    python run_scrapers.py --no-clean        # Executa sem limpar as tabelas
"""

import argparse
import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

from sqlalchemy import text

# Adicionar pasta scrapers ao path
sys.path.append(str(Path(__file__).parent))

from database import SessionLocal, create_tables, get_stats
from scrapers.aerodromos_privados import AerodromosPrivadosScraper
from scrapers.aerodromos_publicos import AerodromosPublicosScraper
from scrapers.municipios_maritimos import MunicipiosMaritimosIBGEScraper
from scrapers.municipios_fronteira import MunicipiosFronteiraIBGEScraper
from scrapers.municipios_suframa import MunicipiosSuframaIBGEScraper
from scrapers.atracacoes_portuarias import AtracacoesPortuariasANTAQScraper
from process_representacoes_fiscais import main as processar_representacoes_fiscais
from utils import cleanup_all_data_files


class BrasilDataHubScrapersManager:
    """Gerenciador principal para todos os scrapers do Brasil Data Hub."""
    
    def __init__(self):
        """Inicializa o gerenciador com todos os scrapers disponÃ­veis."""
        self.scrapers = {
            'private': {
                'name': 'AerÃ³dromos Privados',
                'scraper': AerodromosPrivadosScraper(),
                'description': 'AerÃ³dromos privados registrados na ANAC'
            },
            'public': {
                'name': 'AerÃ³dromos PÃºblicos', 
                'scraper': AerodromosPublicosScraper(),
                'description': 'AerÃ³dromos pÃºblicos registrados na ANAC'
            },
            'maritimos': {
                'name': 'MunicÃ­pios MarÃ­timos',
                'scraper': MunicipiosMaritimosIBGEScraper(),
                'description': 'MunicÃ­pios defrontantes com o mar - IBGE'
            },
            'fronteira': {
                'name': 'MunicÃ­pios de Fronteira',
                'scraper': MunicipiosFronteiraIBGEScraper(),
                'description': 'MunicÃ­pios da faixa de fronteira e cidades gÃªmeas - IBGE'
            },
            'suframa': {
                'name': 'MunicÃ­pios SUFRAMA',
                'scraper': MunicipiosSuframaIBGEScraper(),
                'description': 'MunicÃ­pios das Zonas Fiscais Especiais da SUFRAMA - IBGE'
            },
            'portos': {
                'name': 'AtracaÃ§Ãµes PortuÃ¡rias',
                'scraper': AtracacoesPortuariasANTAQScraper(),
                'description': 'Dados de atracaÃ§Ãµes portuÃ¡rias - ANTAQ'
            },
            'representacoes_fiscais_scraper': {
                'name': 'RepresentaÃ§Ãµes Fiscais (Scraper)',
                'scraper': None,  # SerÃ¡ tratado de forma especial
                'description': 'Coleta de dados de representaÃ§Ãµes fiscais do Power BI'
            },
            'representacoes_fiscais_process': {
                'name': 'RepresentaÃ§Ãµes Fiscais (Processamento)',
                'scraper': None,  # SerÃ¡ tratado de forma especial
                'description': 'Processamento de representaÃ§Ãµes fiscais do CSV para o banco'
            }
        }
        
        # Criar pasta de logs se nÃ£o existir
        Path('logs').mkdir(exist_ok=True)
    
    def create_tables_if_needed(self) -> bool:
        """Cria tabelas se nÃ£o existirem (sem limpÃ¡-las)."""
        print("ðŸ”§ Verificando/criando tabelas no banco de dados...")
        
        try:
            # Criar tabelas se nÃ£o existirem
            create_tables()
            print("âœ… Tabelas verificadas/criadas")
            return True
            
        except Exception as e:
            print(f"âŒ Erro ao verificar/criar tabelas: {e}")
            return False
    
    def run_single_scraper(self, scraper_key: str) -> Dict[str, Any]:
        """Executa um Ãºnico scraper."""
        if scraper_key not in self.scrapers:
            available = ', '.join(self.scrapers.keys())
            raise ValueError(f"Scraper '{scraper_key}' nÃ£o encontrado. DisponÃ­veis: {available}")
        
        scraper_info = self.scrapers[scraper_key]
        print(f"\n{'='*70}")
        print(f"ðŸŽ¯ Executando: {scraper_info['name']}")
        print(f"ðŸ“„ DescriÃ§Ã£o: {scraper_info['description']}")
        print(f"{'='*70}")
        
        start_time = time.time()
        
        try:
            if scraper_key == 'representacoes_fiscais_scraper':
                # Executar o scraper de representaÃ§Ãµes fiscais
                from scrapers.representacoes_fiscais import executar_estrategias_avancadas
                print(f"\n{'='*70}")
                print(f"ðŸŽ¯ Executando: RepresentaÃ§Ãµes Fiscais (Scraper)")
                print(f"ðŸ“„ DescriÃ§Ã£o: Coleta de dados de representaÃ§Ãµes fiscais do Power BI")
                print(f"{'='*70}")
                start_time = time.time()
                try:
                    arquivo_csv = executar_estrategias_avancadas()
                    result = {
                        'success': True,
                        'scraper_name': 'RepresentaÃ§Ãµes Fiscais (Scraper)',
                        'scraper_key': scraper_key,
                        'execution_time': time.time() - start_time,
                        'csv_file': arquivo_csv
                    }
                    print(f"âœ… RepresentaÃ§Ãµes Fiscais (Scraper): ConcluÃ­do com sucesso!")
                except Exception as e:
                    result = {
                        'success': False,
                        'error': str(e),
                        'scraper_name': 'RepresentaÃ§Ãµes Fiscais (Scraper)',
                        'scraper_key': scraper_key,
                        'execution_time': time.time() - start_time
                    }
                    print(f"âŒ RepresentaÃ§Ãµes Fiscais (Scraper): Falhou - {e}")
                return result
            elif scraper_key == 'representacoes_fiscais_process':
                # Executar o processamento do CSV
                print(f"\n{'='*70}")
                print(f"ðŸŽ¯ Executando: RepresentaÃ§Ãµes Fiscais (Processamento)")
                print(f"ðŸ“„ DescriÃ§Ã£o: Processamento de representaÃ§Ãµes fiscais do CSV para o banco")
                print(f"{'='*70}")
                start_time = time.time()
                try:
                    processar_representacoes_fiscais()
                    result = {
                        'success': True,
                        'scraper_name': 'RepresentaÃ§Ãµes Fiscais (Processamento)',
                        'scraper_key': scraper_key,
                        'execution_time': time.time() - start_time
                    }
                    print(f"âœ… RepresentaÃ§Ãµes Fiscais (Processamento): ConcluÃ­do com sucesso!")
                except Exception as e:
                    result = {
                        'success': False,
                        'error': str(e),
                        'scraper_name': 'RepresentaÃ§Ãµes Fiscais (Processamento)',
                        'scraper_key': scraper_key,
                        'execution_time': time.time() - start_time
                    }
                    print(f"âŒ RepresentaÃ§Ãµes Fiscais (Processamento): Falhou - {e}")
                return result
            elif scraper_key == 'representacoes_fiscais':
                print(f"\n{'='*70}")
                print(f"ðŸŽ¯ Executando: RepresentaÃ§Ãµes Fiscais")
                print(f"ðŸ“„ DescriÃ§Ã£o: Processamento de representaÃ§Ãµes fiscais do CSV para o banco")
                print(f"{'='*70}")
                start_time = time.time()
                try:
                    processar_representacoes_fiscais()
                    result = {
                        'success': True,
                        'scraper_name': 'RepresentaÃ§Ãµes Fiscais',
                        'scraper_key': scraper_key,
                        'execution_time': time.time() - start_time
                    }
                    print(f"âœ… RepresentaÃ§Ãµes Fiscais: ConcluÃ­do com sucesso!")
                except Exception as e:
                    result = {
                        'success': False,
                        'error': str(e),
                        'scraper_name': 'RepresentaÃ§Ãµes Fiscais',
                        'scraper_key': scraper_key,
                        'execution_time': time.time() - start_time
                    }
                    print(f"âŒ RepresentaÃ§Ãµes Fiscais: Falhou - {e}")
                return result
            else:
                result = scraper_info['scraper'].run()
                result['scraper_name'] = scraper_info['name']
                result['scraper_key'] = scraper_key
                result['execution_time'] = time.time() - start_time
                
                if result['success']:
                    print(f"âœ… {scraper_info['name']}: ConcluÃ­do com sucesso!")
                else:
                    print(f"âŒ {scraper_info['name']}: Falhou - {result.get('error', 'Erro desconhecido')}")
                
                return result
        except Exception as e:
            error_result = {
                'success': False,
                'error': str(e),
                'scraper_name': scraper_info['name'],
                'scraper_key': scraper_key,
                'execution_time': time.time() - start_time
            }
            print(f"ðŸ’¥ {scraper_info['name']}: ExceÃ§Ã£o fatal - {e}")
            return error_result
    
    def run_all_scrapers(self, clean_tables: bool = True) -> Dict[str, Any]:
        """Executa todos os scrapers em sequÃªncia."""
        print("ðŸš€ Iniciando execuÃ§Ã£o completa dos scrapers do Brasil Data Hub...")
        print(f"ðŸ“… Data/Hora: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        
        start_time = time.time()
        execution_log = {
            'start_time': datetime.now().isoformat(),
            'clean_tables_requested': clean_tables,
            'scrapers_executed': [],
            'summary': {}
        }
        
        # 1. Verificar/criar tabelas (sem limpar)
        if clean_tables:
            print("ðŸ“‹ Modo de limpeza ativado - cada scraper limparÃ¡ sua prÃ³pria tabela apÃ³s validar os novos dados")
        else:
            print("â­ï¸ Modo sem limpeza - novos dados serÃ£o adicionados Ã s tabelas existentes")
        
        # Criar tabelas se necessÃ¡rio
        tables_success = self.create_tables_if_needed()
        execution_log['tables_success'] = tables_success
        
        if not tables_success:
            print("âš ï¸ Falha ao verificar/criar tabelas. Continuando mesmo assim...")
        
        # 2. Executar scrapers
        results = {}
        successful_scrapers = 0
        failed_scrapers = 0
        
        for scraper_key in list(self.scrapers.keys()) + ['representacoes_fiscais_scraper', 'representacoes_fiscais_process']:
            try:
                result = self.run_single_scraper(scraper_key)
                results[scraper_key] = result
                execution_log['scrapers_executed'].append(result)
                
                if result['success']:
                    successful_scrapers += 1
                else:
                    failed_scrapers += 1
                    
            except Exception as e:
                failed_scrapers += 1
                error_result = {
                    'success': False,
                    'error': str(e),
                    'scraper_key': scraper_key,
                    'execution_time': 0
                }
                results[scraper_key] = error_result
                execution_log['scrapers_executed'].append(error_result)
                print(f"ðŸ’¥ Erro crÃ­tico no scraper {scraper_key}: {e}")
        
        # 3. EstatÃ­sticas finais
        total_execution_time = time.time() - start_time
        
        try:
            final_database_stats = get_stats()
        except Exception as e:
            print(f"âš ï¸ Erro ao obter estatÃ­sticas finais do banco: {e}")
            final_database_stats = {}
        
        # 4. Compilar resumo
        total_scrapers = len(self.scrapers) + 2  # +2 para os scrapers de representaÃ§Ãµes fiscais
        execution_summary = {
            'total_scrapers': total_scrapers,
            'successful_scrapers': successful_scrapers,
            'failed_scrapers': failed_scrapers,
            'total_execution_time': total_execution_time,
            'database_stats': final_database_stats,
            'individual_results': results,
            'end_time': datetime.now().isoformat()
        }
        
        execution_log['summary'] = execution_summary
        
        # 5. Salvar log de execuÃ§Ã£o
        self._save_execution_log(execution_log)
        
        # 6. Limpeza final de arquivos brutos
        self._cleanup_raw_files()
        
        # 7. Exibir relatÃ³rio final
        self._print_final_report(execution_summary)
        
        return execution_summary
    
    def _save_execution_log(self, execution_log: Dict[str, Any]) -> None:
        """Salva o log detalhado da execuÃ§Ã£o."""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = f'logs/scrapers_execution_{timestamp}.json'
            
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(execution_log, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ðŸ“‹ Log de execuÃ§Ã£o salvo em: {log_file}")
            
        except Exception as e:
            print(f"âš ï¸ Erro ao salvar log de execuÃ§Ã£o: {e}")
    
    def _cleanup_raw_files(self) -> None:
        """Remove todos os arquivos brutos da pasta data/raw apÃ³s execuÃ§Ã£o completa."""
        print(f"\nðŸ—‘ï¸ Limpando arquivos brutos...")
        
        try:
            raw_path = Path('data/raw')
            if not raw_path.exists():
                print("   ðŸ“ Pasta data/raw nÃ£o existe")
                return
            
            removed_count = 0
            total_size = 0
            
            # Percorrer todos os arquivos na pasta raw
            for file_path in raw_path.iterdir():
                if file_path.is_file() and not file_path.name.startswith('.'):
                    try:
                        file_size = file_path.stat().st_size
                        total_size += file_size
                        file_path.unlink()
                        removed_count += 1
                        print(f"   ðŸ—‘ï¸ Removido: {file_path.name} ({file_size:,} bytes)")
                    except Exception as e:
                        print(f"   âš ï¸ Erro ao remover {file_path.name}: {e}")
            
            if removed_count > 0:
                total_mb = total_size / (1024 * 1024)
                print(f"âœ… Limpeza concluÃ­da: {removed_count} arquivos removidos ({total_mb:.2f} MB)")
            else:
                print("   ðŸ“ Nenhum arquivo bruto encontrado para remoÃ§Ã£o")
                
        except Exception as e:
            print(f"âŒ Erro durante limpeza de arquivos brutos: {e}")
    
    def _print_final_report(self, summary: Dict[str, Any]) -> None:
        """Imprime o relatÃ³rio final da execuÃ§Ã£o."""
        print(f"\n{'='*70}")
        print("ðŸ“ˆ RELATÃ“RIO FINAL DE EXECUÃ‡ÃƒO")
        print(f"{'='*70}")
        
        # Tempo e status geral
        print(f"â±ï¸ Tempo total de execuÃ§Ã£o: {summary['total_execution_time']:.2f} segundos")
        print(f"âœ… Scrapers bem-sucedidos: {summary['successful_scrapers']}/{summary['total_scrapers']}")
        print(f"âŒ Scrapers com falha: {summary['failed_scrapers']}/{summary['total_scrapers']}")
        
        # Resultados individuais
        print(f"\nðŸ“Š Resultados por Scraper:")
        for scraper_key, result in summary['individual_results'].items():
            # Tratar scrapers especiais de representaÃ§Ãµes fiscais
            if scraper_key in ['representacoes_fiscais_scraper', 'representacoes_fiscais_process']:
                scraper_name = result.get('scraper_name', scraper_key)
            else:
                scraper_name = self.scrapers.get(scraper_key, {}).get('name', scraper_key)
            
            status = "âœ… SUCESSO" if result['success'] else "âŒ FALHA"
            execution_time = result.get('execution_time', 0)
            
            print(f"   {status} {scraper_name} ({execution_time:.2f}s)")
            
            if result['success'] and 'stats' in result:
                stats = result['stats']
                
                # EstatÃ­sticas especÃ­ficas por tipo de scraper
                if scraper_key in ['private', 'public']:
                    # AerÃ³dromos - mostrar coordenadas e cÃ³digo OACI
                    print(f"      ðŸ“Š Total: {stats.get('total_aerodromos', 0)}")
                    print(f"      ðŸ“ Com coordenadas: {stats.get('com_coordenadas', 0)}")
                    print(f"      ðŸ·ï¸ Com cÃ³digo OACI: {stats.get('com_codigo_oaci', 0)}")
                elif scraper_key == 'maritimos':
                    # MunicÃ­pios marÃ­timos - mostrar total e Ã¡rea
                    total_mun = stats.get('total_municipios', 0)
                    area_total = stats.get('area_total_km2', 0)
                    print(f"      ðŸ“Š Total: {total_mun} municÃ­pios")
                    if area_total > 0:
                        print(f"      ðŸ“ Ãrea total: {area_total:.2f} kmÂ²")
                elif scraper_key == 'fronteira':
                    # MunicÃ­pios de fronteira - mostrar total, cidades gÃªmeas, etc.
                    total_mun = stats.get('total_municipios', 0)
                    cidades_gemeas = stats.get('cidades_gemeas', 0)
                    toca_limite = stats.get('toca_limite', 0)
                    print(f"      ðŸ“Š Total: {total_mun} municÃ­pios")
                    print(f"      ðŸ¤ Cidades gÃªmeas: {cidades_gemeas}")
                    print(f"      ðŸ”— Tocam limite: {toca_limite}")
                elif scraper_key == 'suframa':
                    # MunicÃ­pios SUFRAMA - mostrar total e distribuiÃ§Ã£o por zona
                    total_mun = stats.get('total_municipios', 0)
                    zonas = stats.get('zonas', [])
                    print(f"      ðŸ“Š Total: {total_mun} municÃ­pios")
                    for zona in zonas:
                        if zona['tipo'] == 'ZONA FRANCA DE MANAUS':
                            print(f"      ðŸ—ï¸ Zona Franca: {zona['count']}")
                        elif zona['tipo'] == 'ÃREAS DE LIVRE COMÃ‰RCIO':
                            print(f"      ðŸ›’ Ãreas Livre ComÃ©rcio: {zona['count']}")
                elif scraper_key == 'portos':
                    # AtracaÃ§Ãµes portuÃ¡rias - mostrar total, coordenadas e top portos
                    total_atr = stats.get('total_atracacoes', 0)
                    com_coords = stats.get('com_coordenadas', 0)
                    print(f"      ðŸ“Š Total: {total_atr} atracaÃ§Ãµes")
                    print(f"      ðŸ“ Com coordenadas: {com_coords}")
                    top_portos = stats.get('top_portos', [])
                    if top_portos:
                        print(f"      ðŸ—ï¸ Top porto: {top_portos[0]['porto']}")
                elif scraper_key in ['representacoes_fiscais_scraper', 'representacoes_fiscais_process']:
                    # RepresentaÃ§Ãµes fiscais - mostrar informaÃ§Ãµes especÃ­ficas
                    if 'csv_file' in result:
                        print(f"      ðŸ“„ Arquivo CSV: {result['csv_file']}")
                    if 'total_registros' in stats:
                        print(f"      ðŸ“Š Total: {stats.get('total_registros', 0)} registros")
            elif not result['success']:
                print(f"      ðŸ’¥ Erro: {result.get('error', 'Desconhecido')}")
        
        # EstatÃ­sticas do banco de dados
        if summary['database_stats']:
            print(f"\nðŸ—„ï¸ EstatÃ­sticas do Banco de Dados:")
            db_stats = summary['database_stats']
            
            if 'aerodromos_privados' in db_stats:
                priv_stats = db_stats['aerodromos_privados']
                print(f"   ðŸ  AerÃ³dromos Privados: {priv_stats['total']}")
                print(f"      ðŸ“ Com coordenadas: {priv_stats['com_coordenadas']}")
                print(f"      ðŸ·ï¸ Com cÃ³digo OACI: {priv_stats['com_codigo_oaci']}")
            
            if 'aerodromos_publicos' in db_stats:
                pub_stats = db_stats['aerodromos_publicos']
                print(f"   ðŸ›ï¸ AerÃ³dromos PÃºblicos: {pub_stats['total']}")
                print(f"      ðŸ“ Com coordenadas: {pub_stats['com_coordenadas']}")
                print(f"      ðŸ·ï¸ Com cÃ³digo OACI: {pub_stats['com_codigo_oaci']}")
            
            if 'municipios_maritimos' in db_stats:
                mar_stats = db_stats['municipios_maritimos']
                print(f"   ðŸŒŠ MunicÃ­pios MarÃ­timos: {mar_stats['total']}")
            
            if 'municipios_fronteira' in db_stats:
                front_stats = db_stats['municipios_fronteira']
                print(f"   ðŸ›ï¸ MunicÃ­pios Fronteira: {front_stats['total']}")
            
            if 'municipios_suframa' in db_stats:
                suframa_stats = db_stats['municipios_suframa']
                print(f"   ðŸ—ï¸ MunicÃ­pios SUFRAMA: {suframa_stats['total']}")
        
        # Status final
        if summary['successful_scrapers'] == summary['total_scrapers']:
            print(f"\nðŸŽ‰ TODOS OS SCRAPERS EXECUTADOS COM SUCESSO!")
        elif summary['successful_scrapers'] > 0:
            print(f"\nâš ï¸ EXECUÃ‡ÃƒO PARCIALMENTE BEM-SUCEDIDA")
        else:
            print(f"\nðŸ’¥ FALHA COMPLETA NA EXECUÃ‡ÃƒO")
        
        print(f"{'='*70}")


def main():
    """FunÃ§Ã£o principal com interface de linha de comando."""
    parser = argparse.ArgumentParser(
        description='Executor de scrapers do Brasil Data Hub',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python run_scrapers.py                                         # Executa todos os scrapers
  python run_scrapers.py --scraper private                       # Apenas aerÃ³dromos privados
  python run_scrapers.py --scraper public                        # Apenas aerÃ³dromos pÃºblicos
  python run_scrapers.py --scraper representacoes_fiscais_scraper # Apenas coleta representaÃ§Ãµes fiscais
  python run_scrapers.py --scraper representacoes_fiscais_process # Apenas processamento representaÃ§Ãµes fiscais
  python run_scrapers.py --no-clean                              # NÃ£o limpa as tabelas antes
  python run_scrapers.py --clean-files                           # Apenas limpa arquivos antigos
        """
    )
    
    parser.add_argument(
        '--scraper',
        choices=['private', 'public', 'maritimos', 'fronteira', 'suframa', 'portos', 'representacoes_fiscais_scraper', 'representacoes_fiscais_process'],
        help='Executa apenas um scraper especÃ­fico'
    )
    
    parser.add_argument(
        '--no-clean',
        action='store_true',
        help='NÃ£o limpa as tabelas antes da execuÃ§Ã£o'
    )
    
    parser.add_argument(
        '--clean-files',
        action='store_true',
        help='Limpa apenas os arquivos antigos sem executar scrapers'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='SaÃ­da mais detalhada'
    )
    
    args = parser.parse_args()
    
    # Criar instÃ¢ncia do gerenciador
    manager = BrasilDataHubScrapersManager()
    
    try:
        if args.clean_files:
            # Executar apenas limpeza de arquivos
            print("ðŸ§¹ Executando limpeza de arquivos antigos...")
            cleanup_all_data_files()
            print("âœ… Limpeza concluÃ­da!")
            sys.exit(0)
        elif args.scraper:
            # Executar apenas um scraper
            # Note: A limpeza agora Ã© feita pelo prÃ³prio scraper apÃ³s validar os dados
            result = manager.run_single_scraper(args.scraper)
            
            # Status de saÃ­da baseado no sucesso
            sys.exit(0 if result['success'] else 1)
        else:
            # Executar todos os scrapers
            summary = manager.run_all_scrapers(clean_tables=not args.no_clean)
            
            # Status de saÃ­da baseado no sucesso geral
            if summary['successful_scrapers'] == summary['total_scrapers']:
                sys.exit(0)  # Sucesso total
            elif summary['successful_scrapers'] > 0:
                sys.exit(2)  # Sucesso parcial
            else:
                sys.exit(1)  # Falha total
                
    except KeyboardInterrupt:
        print("\nðŸ›‘ ExecuÃ§Ã£o interrompida pelo usuÃ¡rio")
        sys.exit(130)
    except Exception as e:
        print(f"\nðŸ’¥ Erro fatal: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
